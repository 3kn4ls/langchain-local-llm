# Gu√≠a de Instalaci√≥n para Raspberry Pi 5

Esta gu√≠a te ayudar√° a ejecutar LangChain + Ollama en tu Raspberry Pi 5 con 8GB de RAM.

---

## √çndice

1. [Requisitos](#requisitos)
2. [Instalaci√≥n R√°pida](#instalaci√≥n-r√°pida)
3. [Instalaci√≥n Manual](#instalaci√≥n-manual)
4. [Modelos Recomendados](#modelos-recomendados)
5. [Uso B√°sico](#uso-b√°sico)
6. [Optimizaci√≥n de Rendimiento](#optimizaci√≥n-de-rendimiento)
7. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
8. [Monitoreo de Recursos](#monitoreo-de-recursos)

---

## Requisitos

### Hardware
- **Raspberry Pi 5** con 8GB de RAM (recomendado)
- Tarjeta microSD de al menos 32GB (se recomiendan 64GB o SSD)
- Fuente de alimentaci√≥n oficial de Raspberry Pi 5 (5V/5A USB-C)
- Sistema de refrigeraci√≥n (disipador o ventilador activo recomendado)

### Software
- **Raspberry Pi OS** (64-bit) - Bookworm o posterior
- **Docker** y **Docker Compose** instalados
- Al menos **10GB de espacio libre** en disco

---

## Instalaci√≥n R√°pida

### 1. Instalar Docker (si no est√° instalado)

```bash
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Agregar tu usuario al grupo docker (evita usar sudo)
sudo usermod -aG docker $USER

# Reiniciar sesi√≥n para aplicar cambios
newgrp docker

# Verificar instalaci√≥n
docker --version
docker compose version
```

### 2. Clonar o descargar este repositorio

```bash
cd ~
git clone <URL_DEL_REPOSITORIO>
cd langchain-local-llm
```

### 3. Ejecutar script de configuraci√≥n autom√°tica

```bash
# Dar permisos de ejecuci√≥n
chmod +x scripts/setup_rpi.sh

# Ejecutar script
./scripts/setup_rpi.sh
```

El script har√° lo siguiente:
- ‚úÖ Verificar requisitos del sistema
- ‚úÖ Configurar variables de entorno
- ‚úÖ Iniciar servicios Docker
- ‚úÖ Descargar modelos LLM (te preguntar√° cu√°l quieres)
- ‚úÖ Verificar que todo funciona

---

## Instalaci√≥n Manual

Si prefieres hacerlo paso a paso:

### 1. Configurar variables de entorno

```bash
# Copiar archivo de configuraci√≥n para RPI
cp .env.rpi .env

# (Opcional) Editar configuraci√≥n
nano .env
```

### 2. Iniciar servicios Docker

```bash
# Iniciar servicios con docker-compose espec√≠fico para RPI
docker compose -f docker-compose.rpi.yml up -d

# Ver logs
docker compose -f docker-compose.rpi.yml logs -f
```

### 3. Esperar a que Ollama est√© listo

```bash
# Verificar que Ollama responde
docker exec ollama-server curl http://localhost:11434/api/tags
```

### 4. Descargar modelos

```bash
# Opci√≥n 1: Gemma 2B (RECOMENDADO para RPI)
docker exec ollama-server ollama pull gemma2:2b

# Opci√≥n 2: Phi-3 Mini
docker exec ollama-server ollama pull phi3:mini

# Opci√≥n 3: Llama 3.2 3B
docker exec ollama-server ollama pull llama3.2:3b

# Modelo de embeddings (para RAG)
docker exec ollama-server ollama pull nomic-embed-text
```

### 5. Verificar instalaci√≥n

```bash
# Listar modelos instalados
docker exec ollama-server ollama list

# Probar el modelo
docker exec ollama-server ollama run gemma2:2b "Hola, ¬øc√≥mo est√°s?"
```

---

## Modelos Recomendados

Para **Raspberry Pi 5 con 8GB de RAM**, estos son los modelos m√°s adecuados:

### üèÜ Gemma 2B (Recomendado)

```bash
docker exec ollama-server ollama pull gemma2:2b
```

**Caracter√≠sticas:**
- **Tama√±o:** ~2.7GB en RAM
- **Calidad:** Excelente para su tama√±o
- **Velocidad:** ~10-15 tokens/segundo en RPI 5
- **Fabricante:** Google
- **Ideal para:** Uso general, chatbots, asistentes

### ü•à Phi-3 Mini

```bash
docker exec ollama-server ollama pull phi3:mini
```

**Caracter√≠sticas:**
- **Tama√±o:** ~2.3GB en RAM
- **Calidad:** Muy bueno en razonamiento
- **Velocidad:** ~12-18 tokens/segundo en RPI 5
- **Fabricante:** Microsoft
- **Ideal para:** C√≥digo, razonamiento l√≥gico

### ü•â Llama 3.2 3B

```bash
docker exec ollama-server ollama pull llama3.2:3b
```

**Caracter√≠sticas:**
- **Tama√±o:** ~2GB en RAM
- **Calidad:** Buena para tareas simples
- **Velocidad:** ~15-20 tokens/segundo en RPI 5
- **Fabricante:** Meta
- **Ideal para:** Respuestas r√°pidas, tareas simples

### ‚ö° TinyLlama (Ultra ligero)

```bash
docker exec ollama-server ollama pull tinyllama
```

**Caracter√≠sticas:**
- **Tama√±o:** ~600MB en RAM
- **Calidad:** B√°sica, pero funcional
- **Velocidad:** ~25-30 tokens/segundo en RPI 5
- **Ideal para:** Pruebas, desarrollo, recursos muy limitados

### üìä Nomic Embed Text (Embeddings para RAG)

```bash
docker exec ollama-server ollama pull nomic-embed-text
```

**Caracter√≠sticas:**
- **Tama√±o:** ~274MB
- **Uso:** Generaci√≥n de embeddings para b√∫squeda sem√°ntica
- **Necesario para:** Sistemas RAG (b√∫squeda en documentos)

---

## Uso B√°sico

### Ejecutar ejemplos interactivos

```bash
# Ejemplos b√°sicos de LangChain
docker exec -it langchain-app python main.py

# Ejemplos de RAG (b√∫squeda en documentos)
docker exec -it langchain-app python rag_example.py

# Ejemplos de agentes con herramientas
docker exec -it langchain-app python agent_example.py
```

### Iniciar API web

```bash
# Reiniciar el contenedor con el servidor API
docker compose -f docker-compose.rpi.yml restart langchain-app
docker compose -f docker-compose.rpi.yml exec -d langchain-app python api_server.py

# La API estar√° disponible en:
# http://<IP_DE_TU_RPI>:8000
```

### Probar la API con curl

```bash
# Health check
curl http://localhost:8000/

# Chat simple
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Explica qu√© es Python en 2 oraciones"}'

# Listar modelos disponibles
curl http://localhost:8000/models
```

### Acceder a shell interactivo de Python

```bash
# Abrir Python dentro del contenedor
docker exec -it langchain-app python

# Luego puedes ejecutar c√≥digo Python:
# >>> from langchain_ollama import ChatOllama
# >>> llm = ChatOllama(model="gemma2:2b")
# >>> response = llm.invoke("Hola")
# >>> print(response.content)
```

---

## Optimizaci√≥n de Rendimiento

### 1. Refrigeraci√≥n

La Raspberry Pi 5 puede calentarse bajo carga intensiva:

```bash
# Ver temperatura actual
vcgencmd measure_temp

# Monitorear temperatura en tiempo real
watch -n 1 vcgencmd measure_temp
```

**Recomendaciones:**
- Temperatura ideal: < 60¬∞C
- Con carga: 60-75¬∞C es normal
- Si supera 80¬∞C: Considera agregar refrigeraci√≥n activa
- Thermal throttling comienza a ~85¬∞C

### 2. Usar SSD en lugar de microSD

Para mejor rendimiento:

```bash
# Los modelos se almacenan en /var/lib/docker/volumes
# Mover Docker a SSD USB mejora significativamente el rendimiento
```

### 3. Limitar uso de RAM

El `docker-compose.rpi.yml` ya incluye l√≠mites de memoria:

```yaml
deploy:
  resources:
    limits:
      memory: 6G  # Ollama: m√°ximo 6GB
```

### 4. Overclock (Avanzado)

**‚ö†Ô∏è ADVERTENCIA:** Overclock puede causar inestabilidad. Solo para usuarios avanzados.

```bash
# Editar /boot/firmware/config.txt
sudo nano /boot/firmware/config.txt

# Agregar:
# arm_freq=2800  # Frecuencia de CPU (2.4GHz por defecto, hasta 3GHz)
# gpu_freq=900   # Frecuencia de GPU
# over_voltage=6 # Voltaje (necesario para overclock)

# Reiniciar
sudo reboot
```

### 5. Desactivar servicios innecesarios

```bash
# Ver servicios activos
systemctl list-units --type=service --state=running

# Desactivar servicios que no necesites
# Ejemplo: Bluetooth si no lo usas
sudo systemctl disable bluetooth
```

---

## Soluci√≥n de Problemas

### Problema 1: "Cannot connect to Docker daemon"

**Error:**
```
Cannot connect to the Docker daemon at unix:///var/run/docker.sock
```

**Soluci√≥n:**
```bash
# Verificar que Docker est√° corriendo
sudo systemctl status docker

# Iniciar Docker si est√° detenido
sudo systemctl start docker

# Verificar que tu usuario est√° en el grupo docker
groups $USER

# Si no est√°, agregarlo:
sudo usermod -aG docker $USER
newgrp docker
```

---

### Problema 2: Contenedor Ollama no inicia

**Ver logs:**
```bash
docker compose -f docker-compose.rpi.yml logs ollama
```

**Posibles soluciones:**
```bash
# Limpiar y reiniciar
docker compose -f docker-compose.rpi.yml down
docker system prune -f
docker compose -f docker-compose.rpi.yml up -d
```

---

### Problema 3: Respuestas muy lentas

**S√≠ntomas:** El modelo tarda mucho en responder

**Soluciones:**

1. **Usar un modelo m√°s ligero:**
```bash
# Cambiar de gemma2:2b a tinyllama
docker exec ollama-server ollama pull tinyllama
# Editar .env y cambiar MODEL_NAME=tinyllama
```

2. **Verificar temperatura:**
```bash
vcgencmd measure_temp
# Si est√° en thermal throttling (>85¬∞C), mejora la refrigeraci√≥n
```

3. **Verificar uso de RAM:**
```bash
free -h
# Si la RAM est√° llena, cierra aplicaciones o usa un modelo m√°s peque√±o
```

---

### Problema 4: Error "Out of Memory"

**S√≠ntomas:** El contenedor se cierra solo o responde con errores

**Soluciones:**

1. **Usar un modelo m√°s peque√±o:**
```bash
# tinyllama usa solo 600MB
docker exec ollama-server ollama pull tinyllama
```

2. **Aumentar swap (memoria virtual):**
```bash
# Ver swap actual
free -h

# Aumentar swap a 4GB
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile
# Cambiar CONF_SWAPSIZE=4096
sudo dphys-swapfile setup
sudo dphys-swapfile swapon
```

3. **Reducir l√≠mites de memoria en docker-compose:**
```yaml
# En docker-compose.rpi.yml
deploy:
  resources:
    limits:
      memory: 4G  # Reducir de 6G a 4G
```

---

### Problema 5: Modelo no encontrado

**Error:**
```
Error: model 'gemma2:2b' not found
```

**Soluci√≥n:**
```bash
# Verificar modelos instalados
docker exec ollama-server ollama list

# Descargar el modelo
docker exec ollama-server ollama pull gemma2:2b

# Verificar nuevamente
docker exec ollama-server ollama list
```

---

### Problema 6: Puerto 8000 ya en uso

**Error:**
```
Bind for 0.0.0.0:8000 failed: port is already allocated
```

**Soluci√≥n:**
```bash
# Ver qu√© proceso usa el puerto 8000
sudo lsof -i :8000

# Opci√≥n 1: Detener el proceso
sudo kill <PID>

# Opci√≥n 2: Cambiar el puerto en docker-compose.rpi.yml
# Editar:
# ports:
#   - "8080:8000"  # Cambiar de 8000 a 8080
```

---

## Monitoreo de Recursos

### Ver uso en tiempo real

```bash
# Uso de CPU, RAM y Red de los contenedores
docker stats

# Informaci√≥n del sistema
htop  # o top si htop no est√° instalado

# Temperatura de la CPU
watch -n 1 vcgencmd measure_temp

# Espacio en disco
df -h
```

### Logs de contenedores

```bash
# Ver logs de todos los servicios
docker compose -f docker-compose.rpi.yml logs -f

# Ver logs solo de Ollama
docker compose -f docker-compose.rpi.yml logs -f ollama

# Ver logs solo de LangChain
docker compose -f docker-compose.rpi.yml logs -f langchain-app

# Ver √∫ltimas 50 l√≠neas
docker compose -f docker-compose.rpi.yml logs --tail=50
```

### Limpieza de espacio

```bash
# Ver espacio usado por Docker
docker system df

# Limpiar im√°genes, contenedores y vol√∫menes no usados
docker system prune -a

# ‚ö†Ô∏è CUIDADO: Esto borra los modelos descargados
docker volume prune

# Listar vol√∫menes
docker volume ls

# Borrar un volumen espec√≠fico
docker volume rm <VOLUME_NAME>
```

---

## Comandos √ötiles

### Gesti√≥n de servicios

```bash
# Iniciar servicios
docker compose -f docker-compose.rpi.yml up -d

# Detener servicios
docker compose -f docker-compose.rpi.yml down

# Reiniciar servicios
docker compose -f docker-compose.rpi.yml restart

# Ver estado de los servicios
docker compose -f docker-compose.rpi.yml ps

# Reconstruir im√°genes (despu√©s de cambios)
docker compose -f docker-compose.rpi.yml build --no-cache
docker compose -f docker-compose.rpi.yml up -d
```

### Gesti√≥n de modelos

```bash
# Listar modelos instalados
docker exec ollama-server ollama list

# Descargar un modelo
docker exec ollama-server ollama pull <modelo>

# Borrar un modelo
docker exec ollama-server ollama rm <modelo>

# Probar un modelo directamente
docker exec -it ollama-server ollama run gemma2:2b "Hola"

# Ver informaci√≥n de un modelo
docker exec ollama-server ollama show gemma2:2b
```

---

## Comparativa de Rendimiento

Velocidades aproximadas en **Raspberry Pi 5 (8GB)** con refrigeraci√≥n activa:

| Modelo | RAM Usada | Tokens/seg | Tiempo de respuesta (50 tokens) |
|--------|-----------|------------|--------------------------------|
| TinyLlama | 600MB | 25-30 | ~2 segundos |
| Llama 3.2 3B | 2GB | 15-20 | ~3-4 segundos |
| Phi-3 Mini | 2.3GB | 12-18 | ~3-5 segundos |
| Gemma 2B | 2.7GB | 10-15 | ~4-6 segundos |

**Notas:**
- Velocidades var√≠an seg√∫n la complejidad del prompt
- Primera ejecuci√≥n es m√°s lenta (carga del modelo)
- Con thermal throttling (>85¬∞C), la velocidad puede reducirse a la mitad

---

## Mejores Pr√°cticas

1. **Usa refrigeraci√≥n activa** - Mantiene rendimiento constante
2. **SSD sobre microSD** - Mejora significativamente la velocidad
3. **Cierra aplicaciones innecesarias** - Libera RAM para el LLM
4. **Usa modelos peque√±os** - Gemma 2B o Phi-3 Mini son ideales
5. **Monitorea recursos** - Usa `docker stats` y `vcgencmd measure_temp`
6. **Habilita swap** - Ayuda cuando la RAM es insuficiente
7. **Actualiza Raspberry Pi OS** - Mejoras de rendimiento constantes

---

## Pr√≥ximos Pasos

Una vez que todo funcione:

1. **Personaliza prompts** - Experimenta con diferentes instrucciones
2. **Crea tu RAG** - Indexa tus propios documentos
3. **Desarrolla una API** - Integra con tus aplicaciones
4. **Automatiza tareas** - Scripts para an√°lisis de textos, etc.

---

## Recursos Adicionales

- **Documentaci√≥n de Ollama:** https://ollama.ai/
- **Documentaci√≥n de LangChain:** https://python.langchain.com/
- **Foro de Raspberry Pi:** https://forums.raspberrypi.com/
- **Comunidad Ollama Discord:** https://discord.gg/ollama

---

## Soporte

Si encuentras problemas:

1. Revisa los logs: `docker compose -f docker-compose.rpi.yml logs`
2. Verifica recursos: `docker stats` y `free -h`
3. Consulta esta documentaci√≥n
4. Abre un issue en el repositorio del proyecto

---

**¬°Disfruta de tu LLM local en Raspberry Pi! üéâü•ß**

*√öltima actualizaci√≥n: 2026-01-06*
