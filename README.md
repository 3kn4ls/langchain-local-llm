# LangChain + Ollama en Docker

Entorno completo para desarrollar con LangChain usando LLMs locales sin costes.

## ðŸš€ Plataformas Soportadas

- **Windows** (Docker Desktop con WSL2)
- **Linux** (x86_64 y ARM64)
- **macOS** (Intel y Apple Silicon)
- **ðŸ¥§ Raspberry Pi 5** (8GB RAM) - [Ver guÃ­a especÃ­fica](RASPBERRY_PI_SETUP.md)

## Requisitos Previos

### Windows / macOS / Linux (x86_64)
- **Docker Desktop** o Docker Engine
- **16 GB RAM** recomendado (8 GB mÃ­nimo)
- **10 GB espacio en disco** para modelos

### Raspberry Pi 5
- **8GB RAM** (recomendado)
- **Docker** instalado
- **32GB+ microSD** o SSD USB
- Ver [RASPBERRY_PI_SETUP.md](RASPBERRY_PI_SETUP.md) para guÃ­a completa

## Inicio RÃ¡pido

### ðŸ¥§ Para Raspberry Pi 5

**Usa la configuraciÃ³n optimizada para ARM64:**

```bash
# InstalaciÃ³n automÃ¡tica (recomendado)
chmod +x scripts/setup_rpi.sh
./scripts/setup_rpi.sh

# O manualmente:
docker compose -f docker-compose.rpi.yml up -d
docker exec ollama-server ollama pull gemma2:2b
```

ðŸ“– **GuÃ­a completa:** [RASPBERRY_PI_SETUP.md](RASPBERRY_PI_SETUP.md)

---

### ðŸ’» Para Windows / macOS / Linux

### 1. Iniciar Ollama

```bash
# Iniciar solo Ollama primero
docker compose up -d ollama

# Verificar que esta corriendo
docker logs ollama-server
```

### 2. Descargar Modelos

```bash
# Modelo principal (4.7 GB)
docker exec ollama-server ollama pull llama3.2

# Modelo de embeddings para RAG (274 MB)
docker exec ollama-server ollama pull nomic-embed-text

# Verificar modelos instalados
docker exec ollama-server ollama list
```

### 3. Iniciar AplicaciÃ³n

```bash
# Iniciar todo
docker compose up -d

# Ver logs
docker compose logs -f langchain-app
```

### 4. Ejecutar Ejemplos

```bash
# Ejemplos bÃ¡sicos
docker exec -it langchain-app python main.py

# Ejemplo RAG
docker exec -it langchain-app python rag_example.py

# Iniciar API REST
docker exec -it langchain-app python api_server.py
```

## Endpoints de la API

Una vez iniciada la API en `http://localhost:8000`:

| Endpoint | Metodo | Descripcion |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/models` | GET | Listar modelos disponibles |
| `/chat` | POST | Chat simple |
| `/chat/stream` | POST | Chat con streaming |
| `/analyze` | POST | Analisis de texto |

### Ejemplo de uso con curl:

```bash
# Chat simple
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola, quien eres?"}'

# Analisis de sentimiento
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "Me encanta este producto!", "task": "sentiment"}'
```

## Modelos Disponibles

### Para PC / Laptop (16GB+ RAM)

| Modelo | TamaÃ±o | RAM Necesaria | Uso Recomendado |
|--------|--------|---------------|-----------------|
| `llama3.2` | 4.7 GB | 16 GB | Uso general |
| `mistral` | 4.1 GB | 16 GB | Buen balance |
| `llama3.1:70b` | 40 GB | 64 GB | Alta calidad |

### Para Raspberry Pi / 8GB RAM

| Modelo | TamaÃ±o | RAM Necesaria | Uso Recomendado |
|--------|--------|---------------|-----------------|
| `gemma2:2b` | 2.7 GB | 6 GB | âœ… Recomendado para RPI |
| `phi3:mini` | 2.3 GB | 6 GB | CÃ³digo y razonamiento |
| `llama3.2:3b` | 2.0 GB | 5 GB | Tareas simples |
| `tinyllama` | 600 MB | 3 GB | Ultra ligero |

Para cambiar de modelo:

```bash
# Descargar nuevo modelo
docker exec ollama-server ollama pull mistral

# Configurar en .env
# MODEL_NAME=mistral
```

## Estructura del Proyecto

```
langchain-local-llm/
â”œâ”€â”€ docker-compose.yml        # ConfiguraciÃ³n para PC/Laptop
â”œâ”€â”€ docker-compose.rpi.yml    # ðŸ¥§ ConfiguraciÃ³n para Raspberry Pi
â”œâ”€â”€ Dockerfile                # Imagen multi-arquitectura
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ .env.example             # Variables de entorno (PC)
â”œâ”€â”€ .env.rpi                 # ðŸ¥§ Variables de entorno (RPI)
â”œâ”€â”€ RASPBERRY_PI_SETUP.md    # ðŸ¥§ GuÃ­a completa para RPI
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Ejemplos bÃ¡sicos
â”‚   â”œâ”€â”€ rag_example.py       # Ejemplo RAG completo
â”‚   â”œâ”€â”€ agent_example.py     # Agentes con herramientas
â”‚   â””â”€â”€ api_server.py        # API REST con FastAPI
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.ps1            # Script Windows
    â””â”€â”€ setup_rpi.sh         # ðŸ¥§ Script para Raspberry Pi
```

## Uso con GPU (NVIDIA)

Si tienes GPU NVIDIA, descomenta las lineas en `docker-compose.yml`:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

Requiere:
- NVIDIA drivers actualizados
- NVIDIA Container Toolkit instalado

## Troubleshooting

### Ollama no responde

```powershell
# Reiniciar contenedor
docker-compose restart ollama

# Ver logs
docker logs ollama-server
```

### Error de memoria

Reduce el modelo o aumenta la memoria de Docker Desktop:
Settings > Resources > Memory

### Modelo no encontrado

```powershell
# Listar modelos disponibles
docker exec ollama-server ollama list

# Descargar modelo faltante
docker exec ollama-server ollama pull <nombre-modelo>
```

## Desarrollo Local (sin Docker)

Si prefieres ejecutar sin Docker:

1. Instalar Ollama nativo: https://ollama.ai
2. Crear entorno virtual:
   ```powershell
   python -m venv venv
   .\venv\Scripts\Activate
   pip install -r requirements.txt
   ```
3. Ejecutar:
   ```powershell
   cd app
   python main.py
   ```
