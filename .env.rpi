# Configuración de entorno para Raspberry Pi 5
# Optimizado para 8GB de RAM

# URL del servidor Ollama (usar 'ollama' como hostname dentro de Docker)
OLLAMA_BASE_URL=http://ollama:11434

# MODELO PRINCIPAL - Elige uno de estos modelos ligeros:
# ========================================================
# Opción 1 (RECOMENDADO): Gemma 2B - Modelo de Google, excelente calidad
MODEL_NAME=gemma2:2b

# Opción 2: Phi-3 Mini - Modelo de Microsoft, muy eficiente
# MODEL_NAME=phi3:mini

# Opción 3: Llama 3.2 3B - Versión ligera de Meta
# MODEL_NAME=llama3.2:3b

# Opción 4: TinyLlama - Ultra ligero, para pruebas
# MODEL_NAME=tinyllama

# MODELO PARA EMBEDDINGS (RAG)
# ========================================================
# nomic-embed-text es ligero (~274MB) y eficiente
EMBEDDING_MODEL=nomic-embed-text

# CONFIGURACIÓN DE LA API
# ========================================================
API_HOST=0.0.0.0
API_PORT=8000

# NIVEL DE LOGGING
# ========================================================
LOG_LEVEL=INFO

# CONFIGURACIÓN DE TEMPERATURA
# ========================================================
# 0.0 = Determinista (para código, análisis)
# 0.7 = Balanceado (conversaciones)
# 1.0+ = Creativo (escritura)
DEFAULT_TEMPERATURE=0.7

# NOTAS:
# ======
# 1. Para cambiar de modelo, primero descárgalo:
#    docker exec ollama-server ollama pull gemma2:2b
#
# 2. Verifica modelos disponibles:
#    docker exec ollama-server ollama list
#
# 3. Uso de RAM estimado:
#    - gemma2:2b: ~2.7GB
#    - phi3:mini: ~2.3GB
#    - llama3.2:3b: ~2GB
#    - nomic-embed-text: ~274MB
#
# 4. Con 8GB de RAM total, se recomienda:
#    - Sistema operativo: ~1-2GB
#    - Ollama + Modelo: ~3-4GB
#    - LangChain App: ~1GB
#    - Buffer: ~1-2GB
