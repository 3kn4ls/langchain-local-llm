# Docker Compose para Raspberry Pi 5 (ARM64)
# Optimizado para 8GB de RAM con modelos LLM ligeros

services:
  # Servicio Ollama - LLM Local optimizado para ARM64
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    platform: linux/arm64
    ports:
      - "11434:11434"
    volumes:
      # Persistir modelos descargados
      - ollama_data:/root/.ollama
    environment:
      # Limitar threads para optimizar en RPI
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    deploy:
      resources:
        limits:
          # Limitar uso de RAM (dejar espacio para el sistema)
          memory: 6G
        reservations:
          memory: 2G
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Servicio LangChain App
  langchain-app:
    build:
      context: .
      dockerfile: Dockerfile
      platforms:
        - linux/arm64
    container_name: langchain-app
    platform: linux/arm64
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - PYTHONUNBUFFERED=1
      # Modelo ligero para Raspberry Pi
      - MODEL_NAME=gemma2:2b
      - EMBEDDING_MODEL=nomic-embed-text
      - API_KEY=local-dev-key
    command: tail -f /dev/null # Mantener contenedor vivo para ejecución manual
    volumes:
      # Montar codigo fuente para desarrollo
      - ./app:/app
      # Persistir base de datos ChromaDB
      - ./chroma_db:/app/chroma_db
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    restart: unless-stopped

volumes:
  ollama_data:
    driver: local

# INSTRUCCIONES DE USO:
# =====================
#
# 1. Iniciar servicios:
#    docker-compose -f docker-compose.rpi.yml up -d
#
# 2. Descargar modelo Gemma 2B (primera vez):
#    docker exec ollama-server ollama pull gemma2:2b
#
# 3. Descargar modelo de embeddings para RAG:
#    docker exec ollama-server ollama pull nomic-embed-text
#
# 4. Ejecutar ejemplos:
#    docker exec -it langchain-app python main.py
#
# 5. Ver logs:
#    docker-compose -f docker-compose.rpi.yml logs -f
#
# 6. Detener servicios:
#    docker-compose -f docker-compose.rpi.yml down
#
# MODELOS RECOMENDADOS PARA RASPBERRY PI 5 (8GB RAM):
# ====================================================
# - gemma2:2b (2.7GB) - Recomendado, excelente calidad/tamaño
# - phi3:mini (2.3GB) - Alternativa de Microsoft
# - llama3.2:3b (2GB) - Versión ligera de Llama
# - tinyllama (600MB) - Muy ligero pero menos capaz
#
# MONITOREO DE RECURSOS:
# =======================
# - Ver uso de memoria: docker stats
# - Temperatura RPI: vcgencmd measure_temp
# - RAM libre del sistema: free -h
